{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_path = '/Users/sandeepsubramanian/CMU/Research/PubMedMining/PubMedCentral/antibody_acknowledgement_sentences_format_fix_antibodies_null_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(sentences_path,'r')\n",
    "sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antibody_names = ['anti-IRF1',\n",
    "                  'anti-IRF-7',\n",
    "                  'anti-HCV core',\n",
    "                  'E1',\n",
    "                  'E2',\n",
    "                  'anti-PIF-antibody',\n",
    "                  'anti-Kar2',\n",
    "                  'anti-Pom152p',\n",
    "                  'T53-1',\n",
    "                  'S5',\n",
    "                  'anti-En',\n",
    "                  'anti-Dll',\n",
    "                  'anti-CD28',\n",
    "                  'DT9',\n",
    "                  'anti-GFAP',\n",
    "                  'anti-tubulin',\n",
    "                  'anti-HA',\n",
    "                  'anti-GFP',\n",
    "                  'anti-IMC1',\n",
    "                  'anti-BiP',\n",
    "                  'anti-dengue',\n",
    "                  'anti-E',\n",
    "                  'anti-ACP',\n",
    "                  'anti-CD63',\n",
    "                  'anti-PCNA',\n",
    "                  'anti-PML',\n",
    "                  'MAGE-A4',\n",
    "                  'anti-NS1',\n",
    "                  'anti-OmpA',\n",
    "                  'anti-MBNL1',\n",
    "                  'anti-Rad53',\n",
    "                  'anti-ACRV1',\n",
    "                  'anti-AMA1',\n",
    "                  'anti-gB',\n",
    "                  'anti-CD4',\n",
    "                  'anti-CD8',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parsed_sentence(sentence,entity):\n",
    "    executable_path = '/usr/local/Cellar/stanford-parser/3.4/bin/lexparser.sh'\n",
    "    os.system(\"echo '\" + sentence + \"' > stanfordtemp.txt\")\n",
    "    parser_out = os.popen(executable_path + ' stanfordtemp.txt').readlines()\n",
    "    parser_out = [i.strip() for i in parser_out if len(i) != 1]\n",
    "    bracketed_parse = \" \".join([i.strip() for i in parser_out if i.strip()[0] == \"(\"])\n",
    "    indices = [root.start() for root in re.finditer('\\(ROOT', bracketed_parse)]\n",
    "    sentences = []\n",
    "    for ind,index in enumerate(indices):\n",
    "        if ind == len(indices) - 1:\n",
    "            end = 9999\n",
    "        else:\n",
    "            end = indices[ind+1]\n",
    "        sentences.append(bracketed_parse[index:end])\n",
    "    print sentences\n",
    "    sentences = [nltk.Tree.fromstring(sent) for sent in sentences if entity in sent.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent = get_parsed_sentence('We thank Dr. Y. Hiromi for supplying B38 (inserted in klingon) and P82 (inserted into deadpan) lines, Dr. M. Inagaki for anti-HA antibody, Mr. S. Fukui and Dr. O. Suyari for preparation of dMcm10 antibody.','anti-HA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AcknowledgementsThe authors gratefully acknowledge Dr. E. Stellacci for providing anti-IRF1, anti-IRF-7 antibodies and positive controls for immunoblotting and to Dr. A. Battistini for helpful discussion; Dr. T. Miyamura, Y. Matsuura and T. Harada for providing anti-HCV core, E1 and E2 monoclonal antibodies, constructs for transfection and for precious suggestions and discussion; S. Tocchio and R. Tomasetto for editorial and secretarial assistance; R. Gilardi for graphical assistance; Dr. E. Pozzi for statistical analysis. This work was performed as part of the IRCCS Ospedale Maggiore di Milano and Istituto Superiore di Sanit&#x000e0; \"Integrated National Project for the Study, the Prevention and the Treatment of the Chronic Hepatology\" (no. H95, Experimental Models line) and it was also funded by the project of the Istituto Superiore di Sanit&#x000e0; \"Viral and Host Factors on Hepatitis Viruses Pathogenesis\" (no. C3NC).\n",
      "\n",
      "\n",
      "[]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'subtrees'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3e0f08318ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mparsed_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_parsed_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msubtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NP'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mphrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'subtrees'"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "CONTEXT_SIZE = 4\n",
    "left_contexts = []\n",
    "right_contexts = []\n",
    "phrases = []\n",
    "for sentence in sentences:\n",
    "    print sentence\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    unique_tokenized_sentence = set(tokenized_sentence)\n",
    "    unique_antibody_names = set(antibody_names)\n",
    "    overlap = unique_tokenized_sentence & unique_antibody_names\n",
    "    if overlap != []:\n",
    "        for item in overlap:\n",
    "            parsed_sentence = get_parsed_sentence(sentence,item)\n",
    "            for ind,subtree in enumerate(list(reversed(parsed_sentence.subtrees()))):\n",
    "                if subtree.label() == 'NP' and ind >= 2:\n",
    "                    phrase = subtree.leaves()\n",
    "                    break\n",
    "            phrases.append(phrase.replace(item,'??'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = 'We thank Dr. Kazuko Nishikura (Wistar Institute) for suggesting the experiments examining the effect of global inhibition of gene expression on parasite motility activation and egress; Dr. Con Beckers (University of North Carolina, Chapel Hill) for advice on 35S labeling experiments; Dr. Claire Walczak, Dr. Jim Powers and members of the Walczak lab (Indiana University) for many helpful discussions during the course of these experiments; Drs. Stephanie Ems-McClung (Indiana University) and Anna Kashina (University of Pennsylvania) for critical readings of the manuscript. We thank Dr. Gary Ward (University of Vermont) for the mouse anti-IMC1 antibody; Dr. Vern Carruthers (University of Michigan) for the rabbit anti-TgMIC2 and rabbit anti-TgPLP1 antibodies; Dr. Con Beckers (University of North Carolina, Chapel Hill) for the rabbit anti-GAP45 and rabbit anti-IMC1 antibodies; Drs. Gusti Zeiner, Michael Reese and John Boothroyd at Stanford University for the plasmid PTKO; Dr. John Murray (University of Pennsylvania) for the ptub-mCherryFP-TubA1 and the pmin-FLAG-TubA1 plasmids; Dr. Wolfgang Mages for pQE30-DIP13 (Universit&#x000e4;t Regensburg, Germany); and Tom Sladewski and Dr. Pat Foster (Indiana University) for the pET22b-FLAG-PolB plasmid.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "executable_path = '/usr/local/Cellar/stanford-parser/3.4/bin/lexparser.sh'\n",
    "os.system(\"echo '\" + sentence + \"' > stanfordtemp.txt\")\n",
    "parser_out = os.popen(executable_path + ' stanfordtemp.txt').readlines()\n",
    "parser_out = [i.strip() for i in parser_out if len(i) != 1]\n",
    "bracketed_parse = \" \".join([i.strip() for i in parser_out if i.strip()[0] == \"(\"])\n",
    "indices = [root.start() for root in re.finditer('\\(ROOT', bracketed_parse)]\n",
    "sentences = []\n",
    "for ind,index in enumerate(indices):\n",
    "    if ind == len(indices) - 1:\n",
    "        end = 9999\n",
    "    else:\n",
    "        end = indices[ind+1]\n",
    "    sentences.append(bracketed_parse[index:end])\n",
    "sentences = [nltk.Tree.fromstring(sent) for sent in sentences if 'antibodies' in sent.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'generator'>\n"
     ]
    }
   ],
   "source": [
    "trial_sentence = sentences[0]\n",
    "print type(trial_sentence.subtrees())\n",
    "subtrees = []\n",
    "for subtree in trial_sentence.subtrees():\n",
    "    if 'BAK1-CT' in subtree.leaves():\n",
    "        print subtree\n",
    "        print '----------'\n",
    "        subtrees.append(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parsed_sentence(sentence,entity):\n",
    "    executable_path = '/usr/local/Cellar/stanford-parser/3.4/bin/lexparser.sh'\n",
    "    os.system(\"echo '\" + sentence + \"' > stanfordtemp.txt\")\n",
    "    parser_out = os.popen(executable_path + ' stanfordtemp.txt').readlines()\n",
    "    parser_out = [i.strip() for i in parser_out if len(i) != 1]\n",
    "    bracketed_parse = \" \".join([i.strip() for i in parser_out if i.strip()[0] == \"(\"])\n",
    "    indices = [root.start() for root in re.finditer('\\(ROOT', bracketed_parse)]\n",
    "    sentences = []\n",
    "    for ind,index in enumerate(indices):\n",
    "        if ind == len(indices) - 1:\n",
    "            end = 9999\n",
    "        else:\n",
    "            end = indices[ind+1]\n",
    "        sentences.append(bracketed_parse[index:end])\n",
    "    sentences = [nltk.Tree.fromstring(sent) for sent in sentences if entity in sent][0]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = get_parsed_sentence('Dr. TT Sun is acknowledged for his kind gift of the anti-pan-UP antibody.','anti-pan-UP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "unique_tokenized_sentence = set(tokenized_sentence)\n",
    "unique_antibody_names = set(antibody_names)\n",
    "overlap = unique_tokenized_sentence & unique_antibody_names\n",
    "overlap = list(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_parsed_sentence(sentence,overlap[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = open('phrases.txt','r')\n",
    "h = open('iter_1_results_antibody_name.tsv','w')\n",
    "phrases = pickle.load(g)\n",
    "temp_phrases = ['the ??']\n",
    "new_words = []\n",
    "new_phrases = []\n",
    "sentence = 'We thank someone for the anti-MAGE antibodies'\n",
    "sentence = nltk.word_tokenize(sentence)\n",
    "phrases = list(set(phrases))\n",
    "unwanted_phrases = ['the ??','??']\n",
    "phrases = [i for i in phrases if i not in unwanted_phrases]\n",
    "for sent_no,sentence in enumerate(sentences):\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.replace('Acknowledgments','')\n",
    "    sentence = sentence.replace('ACKNOWLEDGMENTS','')\n",
    "    sentence = sentence.replace('acknowledgments','')\n",
    "    sentence = sentence.replace('Acknowledgements','')\n",
    "    sentence = sentence.replace('ACKNOWLEDGEMENTS','')\n",
    "    sentence = sentence.replace('acknowledgements','')\n",
    "    sentence = sentence.replace('Acknowledgment','')\n",
    "    sentence = sentence.replace('ACKNOWLEDGMENT','')\n",
    "    sentence = sentence.replace('acknowledgment','')\n",
    "    sentence = sentence.replace('Acknowledgement','')\n",
    "    sentence = sentence.replace('ACKNOWLEDGEMENT','')\n",
    "    sentence = sentence.replace('acknowledgement','')\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    for phrase in phrases:\n",
    "        phrase = phrase.split()\n",
    "        phrase_position = 0\n",
    "        new_word = ''\n",
    "        for word in sentence:\n",
    "            if phrase[phrase_position] == '??':\n",
    "                new_word = word\n",
    "                if phrase_position == len(phrase) - 1:\n",
    "                    h.write(str(sent_no) + '\\t' + new_word + '\\t' + ' '.join(phrase) + '\\t' + ' '.join(sentence))\n",
    "                    h.write('\\n')\n",
    "                    new_words.append(new_word)\n",
    "                    new_phrases.append(' '.join(phrase))\n",
    "                    phrase_position = 0\n",
    "                    continue\n",
    "                phrase_position += 1\n",
    "            elif word == phrase[phrase_position]:\n",
    "                if phrase_position == len(phrase) - 1:\n",
    "                    h.write(str(sent_no) + '\\t' + new_word + '\\t' + ' '.join(phrase) + '\\t' + ' '.join(sentence))\n",
    "                    h.write('\\n')\n",
    "                    new_words.append(new_word)\n",
    "                    new_phrases.append(' '.join(phrase))\n",
    "                    phrase_position = 0\n",
    "                    continue\n",
    "                phrase_position += 1\n",
    "            else:\n",
    "                phrase_position = 0\n",
    "                new_word = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anti-GRA3 , anti-RON4 , and ?? antibodies',\n",
       " 'the polyclonal ?? antibody',\n",
       " '?? deletion virus',\n",
       " 'ChIP grade ?? antibody',\n",
       " 'F , S2 , S3 , ?? , and S6A',\n",
       " 'the ?? and mCherry antibodies',\n",
       " 'anti-HCV core , E1 and ?? monoclonal antibodies',\n",
       " 'the ?? antibody',\n",
       " 'the anti-ERD2 and ?? antibodies',\n",
       " 'the ?? and S5 antibodies',\n",
       " 'the mouse ?? antibody',\n",
       " '?? antibody and members',\n",
       " 'the monoclonal antibody ?? virus type',\n",
       " 'anti-IRF1 , ?? antibodies',\n",
       " '?? , 62 , MSH3 and MSH10',\n",
       " 'The ?? monoclonal antibody',\n",
       " 'anti-AbdB and ?? antibodies',\n",
       " 'the Deutsche Forschungsgemeinschaft , SFB490 individual projects ??',\n",
       " '?? enzymes',\n",
       " 'the depleting ?? -LRB- human recombinant , cM-807 -RRB- antibodies',\n",
       " 'HCV ??',\n",
       " 'the polyclonal rabbit ?? antisera',\n",
       " 'the T53-1 and ?? antibodies',\n",
       " 'rabbit ?? polyclonal antibody',\n",
       " 'the ?? Western blot',\n",
       " 'the goat ?? serum affinity-purification',\n",
       " 'TuJ1 ?? antibodies',\n",
       " '?? polyclonal antibody',\n",
       " '?? , anti-IRF-7 antibodies',\n",
       " 'anti-HCV core , ?? and E2 monoclonal antibodies',\n",
       " 'NS5A and ?? antibodies',\n",
       " '?? inhibitor',\n",
       " 'the EL7 ?? antibody',\n",
       " 'several ?? antibodies',\n",
       " 'rabbit ?? antibodies',\n",
       " '?? antibody A2764',\n",
       " '?? monoclonal antibodies',\n",
       " 'the ?? antibody 29-29',\n",
       " 'the serum ?? measurements',\n",
       " 'HCV ?? protein',\n",
       " 'the rabbit ?? antibody',\n",
       " 'monoclonal ?? antibody',\n",
       " '?? antibody',\n",
       " 'the GK1 .5 -LRB- ?? -RRB-',\n",
       " 'the ?? antibodies',\n",
       " 'the ?? monoclonal',\n",
       " 'recombinant tubulin tyrosine ligase and ?? antibodies',\n",
       " 'monoclonal ?? antibodies',\n",
       " 'The ?? antibodies',\n",
       " '?? antibody 5E10',\n",
       " 'anti-Myc and ?? monoclonal antibodies',\n",
       " '?? -LRB- z -RRB- antibodies',\n",
       " '?? and anti-DsbA antibodies',\n",
       " 'the affinity purified ?? antibody',\n",
       " '?? polyclonal antibodies',\n",
       " '6.72 -LRB- ?? -RRB- monoclonal antibodies',\n",
       " 'the ?? and anti-VP antibodies',\n",
       " '?? antibody .',\n",
       " '?? rabbit polyclonal antibodies',\n",
       " 'the monoclonal antibody ??',\n",
       " 'the ?? antibody and editorial assistance',\n",
       " 'the ?? and anti-ste11 antibodies',\n",
       " 'The ?? antibody',\n",
       " 'rabbit JEV polyclonal C0863 anti-NS1 and C0859 ?? antibodies',\n",
       " 'HCV ?? -LRB- AR3A -RRB- antibody',\n",
       " 'the rat ?? antibody',\n",
       " '?? antibody.This work',\n",
       " 'the anti-cyclin ?? antibody',\n",
       " '?? -LRB- Pc -RRB- antibodies',\n",
       " '?? , anti-IcsA , anti-IpaB - IpaC antibodies',\n",
       " 'the core , ?? , NS2 and NS5A monoclonal antibodies',\n",
       " 'rabbit JEV polyclonal C0863 ?? and C0859 anti-E antibodies',\n",
       " '?? antibodies',\n",
       " 'the ?? monoclonal antibody',\n",
       " 'the mouse monoclonal antibody ??',\n",
       " 'the ?? Dectection antibody']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Number : 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-c28eb875be6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Sentence Number : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokenized_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0munique_tokenized_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0munique_antibody_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mantibody_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk-3.0.0-py2.7.egg/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     91\u001b[0m     along with :class:`.PunktSentenceTokenizer`).\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     return [token for sent in sent_tokenize(text)\n\u001b[0m\u001b[1;32m     94\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk-3.0.0-py2.7.egg/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m     81\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk-3.0.0-py2.7.egg/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \"\"\"\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk-3.0.0-py2.7.egg/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \"\"\"\n\u001b[0;32m-> 1318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk-3.0.0-py2.7.egg/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk-3.0.0-py2.7.egg/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \"\"\"\n\u001b[1;32m   1347\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk-3.0.0-py2.7.egg/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \"\"\"\n\u001b[1;32m    353\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk-3.0.0-py2.7.egg/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "antibody_names.extend(new_words)\n",
    "for ind,sentence in enumerate(sentences):\n",
    "    print 'Sentence Number : ' + str(ind)\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    unique_tokenized_sentence = set(tokenized_sentence)\n",
    "    unique_antibody_names = set(antibody_names)\n",
    "    overlap = unique_tokenized_sentence & unique_antibody_names\n",
    "    overlap = list(overlap)\n",
    "    if overlap != []:\n",
    "        for item in overlap:\n",
    "            parsed_sentence = get_parsed_sentence(sentence,item)\n",
    "            for ind,subtree in enumerate(list(reversed(list(parsed_sentence.subtrees())))):\n",
    "                if subtree.label() == 'NP' and ind >= 2 and item in subtree.leaves():\n",
    "                    phrase = subtree.leaves()\n",
    "                    break\n",
    "            phrases.append(' '.join(phrase).replace(item,'??'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
